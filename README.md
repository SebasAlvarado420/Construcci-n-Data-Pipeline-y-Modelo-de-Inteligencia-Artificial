Construcción Completa de un Data Pipeline y Aplicación de un Modelo de Inteligencia Artificial


Este proyecto surge como parte de nuestro trabajo en grupo para el curso de Administración de Datos, impartido por el profesor Alejandro Zamora. A lo largo del desarrollo, nuestro objetivo fue demostrar la capacidad de construir un flujo de datos completo: desde la extracción automatizada de información hasta la implementación y despliegue de un modelo de inteligencia artificial avanzado, integrado en una aplicación web interactiva.

La primera etapa del proyecto se centró en diseñar un Data Pipeline robusto. Iniciamos descargando el dataset de enfermedades cardíacas utilizando la API de Kaggle, lo cual nos permitió obtener de manera automática el archivo comprimido del dataset “heart-disease-dataset”. A partir de ese archivo, extrajimos el CSV y lo renombramos como data_original.csv. Luego, aplicamos un proceso de limpieza en el que eliminamos filas con datos faltantes y duplicados, generando un archivo intermedio (data.csv) que contenía únicamente datos de calidad. Dado que en nuestro contexto el dato “age” resulta fundamental y no es considerado sensible, optamos por omitir cualquier proceso de cifrado sobre las variables, asegurando que la integridad de la información se mantuviera para el análisis posterior.

Una vez obtenido el conjunto de datos final, lo exportamos como data_analisis.csv y subimos este archivo a AWS S3, utilizando la encriptación en reposo (SSE-S3) para cumplir con el requisito de proteger y almacenar los datos en la nube. Este paso fue crucial para demostrar que nuestro pipeline no solo prepara y limpia los datos, sino que también implementa buenas prácticas de seguridad y almacenamiento en entornos cloud.

Con los datos listos, avanzamos a la etapa de Machine Learning. Entrenamos un modelo de Random Forest configurado con parámetros cuidadosamente seleccionados (100 árboles, criterio "entropy", profundidad máxima de 7, entre otros) para predecir la presencia de enfermedad cardíaca. Utilizando una separación adecuada de los datos en conjuntos de entrenamiento y prueba, logramos un rendimiento notable, obteniendo una precisión en el conjunto de entrenamiento superior al 97% y una precisión realista en el conjunto de prueba, alrededor del 79%. Estos resultados reflejan la capacidad del modelo para generalizar, a pesar de la reducción en la precisión cuando se evalúa con datos no vistos durante el entrenamiento.

Finalmente, para demostrar la aplicabilidad práctica de nuestro modelo, desarrollamos una aplicación web con Streamlit. Esta app permite a los usuarios ingresar las características relevantes de un paciente –como edad, sexo, tipo de dolor en el pecho, presión arterial, colesterol, entre otros– y obtener una predicción en tiempo real sobre el riesgo de enfermedad cardíaca. La interfaz fue diseñada para ser intuitiva y visualmente atractiva, integrando gráficos exploratorios que ayudan a comprender la distribución y correlación de las variables en el dataset.

En resumen, este proyecto integra de forma fluida y completa la extracción, limpieza, protección y almacenamiento de datos, el entrenamiento y evaluación de un modelo de inteligencia artificial, y su despliegue en una aplicación web interactiva. Cada etapa ha sido desarrollada pensando en buenas prácticas y en la importancia de construir soluciones escalables y seguras, respondiendo a los requerimientos establecidos por nuestro profesor.
